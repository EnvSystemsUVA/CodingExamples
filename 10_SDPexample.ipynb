{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AZ7ua8RPZA7nClzTQrAB9R1Lxh-uwXaP","timestamp":1711222354288}],"authorship_tag":"ABX9TyO5wBVcNOvq8XB1VvDkbV5O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Stochastic vs. Deterministic Dynamic Programming for reservoir operations\n","\n","## Deterministic Dynamic Programming (DDP)\n","\n","Consider the three-season reservoir operation problem from [DPexample.ipynb](https://colab.research.google.com/github/EnvSystemsUVA/CodingExamples/blob/main/07_DPexample.ipynb), except now, assume the inflow distributions each season are $Y_1=\\ln(Q_1)\\sim N(2.3, 0.2)$, $Y_2=\\ln(Q_2)\\sim N(3.5, 0.3)$, and $Y_3=\\ln(Q_3)\\sim N(3.1, 0.3)$ in seasons 1, 2 and 3, respectively and $\\rho(Y_t,Y_{t+1})=0.5$. Your goal is to minimize the sum across the three seasons of expected squared deviations above and below a constant storage target of 20 and above and below a constant release target of 20.\n","\n","Let's look at the statistics of these flow distributions."],"metadata":{"id":"mvaDahOKNyNA"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import scipy.stats as ss\n","import matplotlib.pyplot as plt\n","\n","# inflow parameters\n","muY = np.array([2.3, 3.5, 3.1]) # log-space mean flows\n","sigmaY = np.array([0.2, 0.3, 0.3]) # log-space standard deviations\n","meanQ = np.exp(muY + 0.5*sigmaY**2) # real-space mean flows\n","rho = 0.5 # correlation between consecutive log-space flows\n","\n","nSeasons = len(muY) # number of seasons\n","for i in range(nSeasons):\n","  print(\"Mean flow, season %d: %0.1f\" % ((i+1), meanQ[i]))\n","  print(\"1st percentile flow, season %d: %0.1f\" % ((i+1), np.exp(ss.norm.ppf(0.01,loc=muY[i],scale=sigmaY[i]))))\n","  print(\"Median flow, season %d: %0.1f\" % ((i+1), np.exp(ss.norm.ppf(0.5,loc=muY[i],scale=sigmaY[i]))))\n","  print(\"99th percentile flow, season %d: %0.1f\" % ((i+1), np.exp(ss.norm.ppf(0.99,loc=muY[i],scale=sigmaY[i]))))\n","  print(\"\\n\")"],"metadata":{"id":"XZgk5UW3kkJ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using deterministic discrete dynamic programming, find the operating policy if you assume you receive the mean inflow every season. Consider 7 discrete storage values: 0, 5, 10, 15, 20, 25, and 30. Assume the releases cannot be less than 10 or greater than 40. Report your results as a table showing the optimal release in each of the three seasons for each of the seven storage levels. Use bounds of [10,40] for releases and [0,30] for storage.\n","\n","This code is the same as in [DPexample.ipynb](https://colab.research.google.com/github/EnvSystemsUVA/CodingExamples/blob/main/07_DPexample.ipynb), it's just using mean flows from the above log-normal distributions (11.0, 38.5, 25.8) instead of (10,50,30)."],"metadata":{"id":"nJwH-rOUNelb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIE7ok8oaphe"},"outputs":[],"source":["# targets\n","Stargets = np.ones([nSeasons])*20 # target of 20 every period\n","Rtargets = np.ones([nSeasons])*20 # target of 20 every period\n","\n","# reservoir parameters\n","K = 30 # reservoir capacity\n","Rmax = 50 # maximum possible release\n","Rmin = 10 # minimum possible release\n","\n","######################## DDP Optimization ########################\n","\n","def calcCostDDP(S, Q, Starget, Rtarget, bounds, FutureCost):\n","  '''\n","  Function to calculate the optimal release (Rbest) from each storage state (S)\n","  and associated present and future cost (Cbest), defined as the total squared\n","  deviation between storage and release targets for that stage.\n","\n","  Inputs:\n","    S: 1-D array of discrete storage values representing the states\n","    Q: inflow received at this stage (scalar)\n","    Starget: target storage for next stage (scalar)\n","    Rtarget: release target for this stage (scalar)\n","    bounds: bounds on possible releases (array of length 2)\n","    FutureCost: 1-D array of future costs at each state that will be added to the\n","      cost of the optimal state transition at this stage to compute present + future cost\n","\n","  Outputs:\n","    Rbest: 1-D array of optimal releases from each state in S\n","    Cbest: 1-D array of present + future costs associated with each state S\n","  '''\n","\n","  # initialize current cost at infinity and releases at 0\n","  Cbest = np.empty([len(S)])+np.inf\n","  Rbest = np.zeros([len(S)])\n","  for i, s in enumerate(S): # storage at stage t\n","    # find optimal storage to move to at stage t+1\n","    for j, sNext in enumerate(S): # storage at stage t+1\n","      R = s + Q - sNext # release to get to sNext\n","      # find cost of this release if it's feasible\n","      if R >= bounds[0] and R <= bounds[1]:\n","        # compute total cost C (total deviation from targets + future cost at sNext)\n","        S_deviation = (sNext-Starget)**2\n","        R_deviation = (R-Rtarget)**2\n","        C = S_deviation + R_deviation + FutureCost[j]\n","        # update optimal value (Cbest) and decision (Rbest) if better than current best\n","        if C < Cbest[i]:\n","          Cbest[i] = C\n","          Rbest[i] = R\n","\n","  return Rbest, Cbest\n","\n","# get indices of stages\n","nStages = nSeasons\n","forward_indices = np.arange(nStages)\n","backward_indices = forward_indices[::-1] # reverse order of stages for backward-moving DP\n","backward_indices = np.insert(backward_indices,0,0) # put 0 at beginning to make it cyclic\n","\n","# discretize states\n","states = np.arange(0,31,5)\n","nStates = len(states)\n","\n","# bounds on decision variables (releases)\n","bounds = np.array([Rmin, Rmax])\n","\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","DDP_costs = np.empty([nStates,nStages])\n","DDP_release_policy = np.empty([nStates,nStages])\n","\n","# initialize FutureCost at 0 for all states; will update as we move backwards\n","FutureCost = np.zeros([nStates])\n","\n","# begin backward-moving DDP\n","loop = True\n","while loop:\n","  count = 0\n","  for index in backward_indices[0:-1]:\n","    # find optimal release and value of each state in this stage\n","    # states (storage targets) are for next period (index) while inflows and releases are for this period (index-1)\n","    R, FutureCost = calcCostDDP(states, meanQ[index-1], Stargets[index], Rtargets[index-1], bounds, FutureCost)\n","\n","    # count iterations with no change in optimal release\n","    if np.all(R == DDP_release_policy[:,index-1]):\n","      count += 1\n","\n","    # update best releases and value of each state\n","    DDP_costs[:,index] = FutureCost\n","    DDP_release_policy[:,index-1] = R\n","\n","  # stop loop if no change in optimal decisions across all iterations\n","  if count == len(backward_indices[0:-1]):\n","    break\n","\n","DDP_release_policy_df = pd.DataFrame(DDP_release_policy, columns=[\"Season 1\",\"Season 2\",\"Season 3\"],index=states)\n","DDP_release_policy_df.index.rename(\"Storage\",inplace=True)\n","DDP_release_policy_df"]},{"cell_type":"markdown","source":["## Stochastic Dynamic Programming (SDP)\n","\n","Now explicitly consider uncertainty in the optimization using stochastic dynamic programming.\n","\n","First, we'll compute the transition probabilities from 10 discrete log-space flow levels $Y_{t-1}$ in season $t-1$ to 10 discrete log-space flow levels $Y_t$ in season $t$. The log-space flows of consecutive seasons follow a bivariate normal distribution:  \n","\n","$f(Y_{t-1}, Y_t) = \\frac{1}{2\\pi \\sigma_{y_{t-1}} \\sigma_{y_t} \\sqrt{1-\\rho^2}} \\exp \\Bigg(- \\frac{1}{2(1-\\rho^2)} \\bigg[ \\bigg(\\frac{y_{t-1}-\\mu_{y_{t-1}}}{\\sigma_{y_{t-1}}}\\bigg)^2 - 2\\rho\\bigg(\\frac{y_{t-1}-\\mu_{y_{t-1}}}{\\sigma_{y_{t-1}}}\\bigg)\\bigg(\\frac{y_t-\\mu_{y_t}}{\\sigma_{y_t}}\\bigg) + \\bigg(\\frac{y_t-\\mu_{y_t}}{\\sigma_{y_t}}\\bigg)^2  \\bigg] \\Bigg)$  \n","\n","From this, the conditional distribution of $Y_t$ given an observed value of $Y_{t-1}$ is normally distributed with conditional mean $\\mu_{y_t|y_{t-1}}$ and conditional standard deviation $\\sigma_{y_t|y_{t-1}}$:  \n","\n","\\begin{align}\n","f(Y_t | Y_{t-1}=y_{t-1}) &= \\frac{1}{\\sigma_{y_t|y_{t-1}}\\sqrt{2\\pi}} \\exp \\bigg( -\\frac{(y_t-\\mu_{y_t|y_{t-1}})^2}{\\sigma_{y_t|y_{t-1}}^2} \\bigg)\\\\\n","\\mu_{y_t|y_{t-1}} &= \\mu_{y_t} + \\rho\\frac{\\sigma_{y_t}}{\\sigma_{y_{t-1}}}\\Big(y_{t-1}-\\mu_{y_{t-1}}\\Big)\\\\\n","\\sigma_{y_t|y_{t-1}} &= \\sigma_{y_t}\\sqrt{(1-\\rho^2)}\n","\\end{align}"],"metadata":{"id":"cQTMnrN_E3Wr"}},{"cell_type":"code","source":["forward_indices = np.append(forward_indices,0)\n","print(forward_indices)\n","for i, index in enumerate(forward_indices[0:-1]):\n","  print(\"First season: \", index+1)\n","  print(\"Next season: \", forward_indices[i+1]+1)"],"metadata":{"id":"CN9NE1w5oOFV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# find transition probabilities from discrete flow values in one season to the next\n","def calcTransProb(mu, sigma, rho, nLevels):\n","  transprob = np.empty([nLevels,nLevels])\n","  # discrete levels of log-space flows\n","  Ylevels1 = np.linspace(ss.norm.ppf(0.01,mu[0],sigma[0]),ss.norm.ppf(0.99,mu[0],sigma[0]),nLevels)\n","  Ylevels2 = np.linspace(ss.norm.ppf(0.01,mu[1],sigma[1]),ss.norm.ppf(0.99,mu[1],sigma[1]),nLevels)\n","  print(\"Log-space flow levels, t-1: \", Ylevels1)\n","  print(\"Log-space flow levels, t: \", Ylevels2)\n","  for i in range(nLevels):\n","    # find conditional distribution of Qlevels2[j] given flow is Qlevels1[i] in previous season\n","    mu_cond = mu[1] + rho*(sigma[1]/sigma[0])*(Ylevels1[i] - mu[0])\n","    sigma_cond = sigma[1] * np.sqrt(1-rho**2)\n","    for j in range(nLevels):\n","      transprob[i,j] = ss.norm.pdf(Ylevels2[j], mu_cond, sigma_cond)\n","\n","    #normalize probabilities to sum to 1\n","    transprob[i,:] = transprob[i,:] / np.sum(transprob[i,:])\n","\n","  return transprob\n","\n","# discretize inflows into nLevels and find transition probabilities between them\n","nLevels=10\n","transprob = []\n","for i, index in enumerate(forward_indices[0:-1]):\n","  print(\"Season \" + str(index+1) + \" to Season \" + str(forward_indices[i+1]+1))\n","  mu = np.array([muY[index],muY[forward_indices[i+1]]]) # log-space mean\n","  sigma = np.array([sigmaY[index],sigmaY[forward_indices[i+1]]]) # log-space standard deviation\n","  transprob.append(calcTransProb(mu, sigma, rho, nLevels))\n","  print(\"Transition probabilities: \", transprob[i])\n","  print(\"\\n\")"],"metadata":{"id":"4Aq_g7iCE3Hj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's plot these transition probabilities to improve the visualization."],"metadata":{"id":"ky1oo-QTi5YD"}},{"cell_type":"code","source":["import matplotlib as mpl\n","\n","max_transprob = 0\n","for i in range(nSeasons):\n","  if np.max(transprob[i]) > max_transprob:\n","    max_transprob = np.max(transprob[i])\n","\n","norm = mpl.colors.Normalize(0,max_transprob)\n","contour_cmap = mpl.cm.get_cmap('viridis')\n","\n","fig, ax = plt.subplots(1,3, layout=\"constrained\")\n","\n","# make a heatmap of the transition probabilities of each stage\n","for i, index in enumerate(forward_indices[0:-1]):\n","  mu = np.array([muY[index],muY[forward_indices[i+1]]]) # log-space mean\n","  sigma = np.array([sigmaY[index],sigmaY[forward_indices[i+1]]]) # log-space standard deviation\n","  Ylevels1 = np.linspace(ss.norm.ppf(0.01,mu[0],sigma[0]),ss.norm.ppf(0.99,mu[0],sigma[0]),nLevels)\n","  Ylevels2 = np.linspace(ss.norm.ppf(0.01,mu[1],sigma[1]),ss.norm.ppf(0.99,mu[1],sigma[1]),nLevels)\n","  x, y = np.meshgrid(Ylevels1, Ylevels2)\n","  cf = ax.flat[i].contourf(x, y, np.transpose(transprob[i]), norm=norm)\n","  ax.flat[i].set_xlabel(r\"$Y_{t-1}$\")\n","  ax.flat[i].set_title(\"Season \" + str(i+1) + \"-\" + str(forward_indices[i+1]+1))\n","  if i == 0:\n","    ax.flat[i].set_ylabel(r\"$Y_t$\")\n","\n","cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=contour_cmap))\n","cbar.ax.set_ylabel(\"Transition Probability\")"],"metadata":{"id":"GQ0Q_3XKi8_Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next we'll modify the DDP function to calculate the costs of present and *expected* future costs across possible inflows $Y_t$ where the probability of those inflows is determined based on the above conditional distribution. Since those inflow probabilities depend on the flow we received in the previous period, we'll now optimize a policy at each stage for the releases as a function of the current storage $S_t$ and the previous inflow $Y_{t-1}$."],"metadata":{"id":"Eo2AycC2XwcY"}},{"cell_type":"code","source":["######################## SDP Optimization ########################\n","from scipy.optimize import minimize\n","\n","def findBestR(R, s, y1, j, S, Ylevels2, transprob, Starget, Rtarget, FutureExpCost):\n","  # compute present cost: squared release deviation\n","  C = (R-Rtarget)**2\n","\n","  # add expected future cost over possible inflows\n","  ExpCost = 0\n","  for m, y2 in enumerate(Ylevels2): # inflow at this stage, t\n","    prob = transprob[j,m] # probability of getting inflow q2 given q1\n","    sNext = s + np.exp(y2) - R # storage we would move to with this inflow exp(y2) and release R\n","\n","    # find closest storage states to sNext and interpolate future cost at those storages\n","    futureCost = np.interp(sNext,S,FutureExpCost[:,j])\n","\n","    # compute future cost C, weighted by the probability of this flow transition\n","    S_deviation = (sNext-Starget)**2\n","    ExpCost += (S_deviation + futureCost) * prob\n","\n","  # add present and expected future cost\n","  C += ExpCost\n","\n","  return C\n","\n","def calcCostSDP(S, Ylevels1, Ylevels2, Qguess, transprob, Starget, Rtarget, bounds, FutureExpCost):\n","  '''\n","  Function to calculate the optimal release (Rbest) from each storage state (S)\n","  and associated present and future cost (Cbest), defined as the total squared\n","  deviation between storage and release targets for that stage.\n","\n","  Inputs:\n","    S: 1-D array of discrete storage values representing the states\n","    Qlevels1: 1-D array of discrete inflow levels in the previous stage\n","    Qlevels2: 1-D array of discrete inflow levels in this stage\n","    transprob: 2-D array of transition probabilities from Qlevels1 to Qlevels2\n","    Starget: target storage for next stage (scalar)\n","    Rtarget: release target for this stage (scalar)\n","    bounds: bounds on possible releases (array of length 2)\n","    FutureExpCost: 2-D array of future expected costs at each combination of S and Qlevels1\n","      that will be added to the cost of the optimal state transition at this stage\n","      to compute present + expected future cost\n","\n","  Outputs:\n","    Rbest: 2-D array of optimal releases as a function of S and Qlevels1\n","    Cbest: 2-D array of present + expected future costs associated with each\n","      combination of S and Qlevels1\n","  '''\n","\n","  # initialize current cost at infinity and releases at 0\n","  Cbest = np.empty([len(S),len(Ylevels1)])+np.inf\n","  Rbest = np.zeros([len(S),len(Ylevels1)])\n","  for i, s in enumerate(S): # storage at stage t\n","    for j, y1 in enumerate(Ylevels1): # inflow at previous stage (t-1)\n","      # find optimal release at this stage\n","      # use mean inflow as an initial guess\n","      # constrain it to not exceed reservoir capacity at mean inflow\n","      constraints = ({'type': 'ineq', 'fun': lambda x: K - (s + Qguess - x[0])}, # capacity - Snext >= 0\n","        {'type': 'ineq', 'fun': lambda x: s + Qguess - x[0]}) # Snext >= 0\n","      result = minimize(findBestR, x0=Qguess, bounds=[[bounds[0],bounds[1]]], constraints=constraints,\n","                        args=(s, y1, j, S, Ylevels2, transprob, Starget, Rtarget, FutureExpCost))\n","      R = result.x[0]\n","      C = result.fun\n","\n","      # update optimal value (Cbest) and decision (Rbest) if better than current best\n","      if C < Cbest[i,j]:\n","        Cbest[i,j] = C\n","        Rbest[i,j] = R\n","\n","  return Rbest, Cbest"],"metadata":{"id":"SXZmkrKJl97M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use same stages, states, indices and bounds as for DDP\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","SDP_costs = np.empty([nStates, nLevels, nStages])\n","SDP_release_policy = np.empty([nStates, nLevels, nStages])\n","\n","# initialize FutureCost at 0 for all states; will update as we move backwards\n","FutureExpCost = np.zeros([nStates, nLevels])\n","\n","# begin backward-moving SDP\n","tolerance = 0.1# average % difference in optimal releases from one cycle to the next at which to stop looping\n","avgPctDiff = np.inf\n","cycle = 0 # number of times cycling through all seasons\n","while loop:\n","  count = 0\n","  cycle +=1\n","  print(cycle)\n","  for index in backward_indices[0:-1]:\n","    mu = np.array([muY[index-2],muY[index-1]])\n","    sigma = np.array([sigmaY[index-2],sigmaY[index-1]])\n","    Ylevels1 = np.linspace(ss.norm.ppf(0.01,mu[0],sigma[0]),ss.norm.ppf(0.99,mu[0],sigma[0]),nLevels)\n","    Ylevels2 = np.linspace(ss.norm.ppf(0.01,mu[1],sigma[1]),ss.norm.ppf(0.99,mu[1],sigma[1]),nLevels)\n","\n","    # find optimal release and value of each state in this stage\n","    # states (storage targets) are for next period (index) while inflows and releases are for this period (index-1)\n","    R, FutureExpCost = calcCostSDP(states, Ylevels1, Ylevels2, meanQ[index-1], transprob[index-1], Stargets[index], Rtargets[index-1], bounds, FutureExpCost)\n","\n","    # find average % difference in optimal releases at this stage compared to the last loop\n","    if cycle > 1:\n","      avgPctDiff = np.mean(np.abs(R - SDP_release_policy[:,:,index-1])*100/SDP_release_policy[:,:,index-1])\n","      if avgPctDiff < tolerance:\n","        count += 1\n","\n","    # update best releases and value of each state\n","    SDP_costs[:,:,index] = FutureExpCost\n","    SDP_release_policy[:,:,index-1] = R\n","\n","  # stop loop if average % change in optimal decisions across all iterations < tolerance\n","  if count == len(backward_indices[0:-1]):\n","    break\n","\n","# print the release policy of each stage\n","for i in range(nSeasons):\n","  cols = []\n","  Ylevels1 = np.linspace(ss.norm.ppf(0.01,muY[i-1],sigmaY[i-1]),ss.norm.ppf(0.99,muY[i-1],sigmaY[i-1]),nLevels)\n","  for j in range(nLevels):\n","    cols.append(\"Q = %0.2f\" % np.exp(Ylevels1[j]))\n","  SDP_release_policy_df = pd.DataFrame(SDP_release_policy[:,:,i], columns=cols,index=states)\n","  SDP_release_policy_df.index.rename(\"Storage\",inplace=True)\n","  print(\"Season \" + str(i+1) + \" release policy\", SDP_release_policy_df)"],"metadata":{"id":"K-my3L_LmAXJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, let's see the release policy visually with a heat map."],"metadata":{"id":"kgKXTg4pnY3a"}},{"cell_type":"code","source":["norm = mpl.colors.Normalize(Rmin, 40)\n","fig, ax = plt.subplots(1,3, layout=\"constrained\")\n","\n","# make a heatmap of the release policy each stage\n","for i, index in enumerate(forward_indices[0:-1]):\n","  mu = np.array([muY[index],muY[forward_indices[i+1]]]) # log-space mean\n","  sigma = np.array([sigmaY[index],sigmaY[forward_indices[i+1]]]) # log-space standard deviation\n","  Ylevels1 = np.linspace(ss.norm.ppf(0.01,mu[0],sigma[0]),ss.norm.ppf(0.99,mu[0],sigma[0]),nLevels)\n","  x, y = np.meshgrid(states, Ylevels1)\n","  cf = ax.flat[i].contourf(x, y, np.transpose(SDP_release_policy[:,:,i]), norm=norm)\n","  ax.flat[i].set_xlabel(r\"$S_t$\")\n","  ax.flat[i].set_title(\"Season \" + str(i+1) + \"-\" + str(forward_indices[i+1]+1))\n","  if i == 0:\n","    ax.flat[i].set_ylabel(r\"$Y_{t-1}$\")\n","\n","cbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=contour_cmap))\n","cbar.ax.set_ylabel(\"Release\")\n","fig.suptitle(\"SDP Release Policy\")"],"metadata":{"id":"-a7UBXsgnb1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Simulation\n","\n","Simulate 50 years of operations in which the release each season is determined by the DDP operating policy found to be optimal in part (a) vs. the SDP operating policy found to be optimal in part (b). In both cases, if there is insufficient water to meet the release prescribed by the policy, only release as much water as is available. Likewise, if the prescribed release would result in exceeding the reservoir capacity, release as much as needed to prevent that. This may result in the release falling below 10 or exceeding 40, but that's okay for the purpose of this simulation.\n","\n","This code is the same as in [DPexample.ipynb](https://colab.research.google.com/github/EnvSystemsUVA/CodingExamples/blob/main/07_DPexample.ipynb)."],"metadata":{"id":"cxc9BTJOTg7I"}},{"cell_type":"code","source":["# initialize storages and releases for simulation of 50 years of 3 seasons with NLP and DP policies\n","nYears = 50\n","\n","class Solution():\n","  def __init__(self):\n","    '''initialize Solution class with certain attributes for DP vs. NLP solution'''\n","    self.simS = np.zeros([nYears,nSeasons])\n","    self.simR = np.zeros([nYears,nSeasons])\n","    self.S_costs = np.zeros([nYears])\n","    self.R_costs = np.zeros([nYears])\n","    self.Total_costs = np.zeros([nYears])\n","    self.prescribedR = None\n","    self.Rmin_violations = 0\n","    self.Rmax_violations = 0\n","\n","  def getSimStates(self, Q, year, season):\n","    '''method of Solution class to calculate simulated R and S'''\n","    # adjust prescribed release if not physically possible\n","    # R = min(prescribedR, simS + Q) prevents it from releasing more water than is available\n","    # max(simS + Q - K, R) prevents storage capacity from being exceeded\n","    self.simR[year,season] = max(self.simS[year,season] + Q - K,\n","                           min(self.prescribedR, self.simS[year,season] + Q))\n","\n","    # count the number of violations of Rmin or Rmax\n","    if self.simR[year,season] > Rmax:\n","      self.Rmax_violations += 1\n","    elif self.simR[year, season] < Rmin:\n","      self.Rmin_violations += 1\n","\n","    # calculate new storage\n","    if season != (nSeasons-1): # storage in next season of same year\n","      self.simS[year,season+1] = self.simS[year,season] + Q - self.simR[year,season]\n","    elif year != (nYears-1): # storage in season 1 of next year\n","      self.simS[year+1,0] = self.simS[year,season] + Q - self.simR[year,season]\n","\n","  def getSimCosts(self, year):\n","    '''method of Solution class to calculate cost (total deviation from targets) over simulation'''\n","    self.S_costs[year] = np.sum((self.simS[year,:] - Stargets)**2)\n","    self.R_costs[year] = np.sum((self.simR[year,:] - Rtargets)**2)\n","    self.Total_costs[year] = self.S_costs[year] + self.R_costs[year]"],"metadata":{"id":"56pNFTrGOx26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.interpolate import RegularGridInterpolator as interp2d\n","\n","# create objects of Solution class for DDP and SDP solutions\n","DDP = Solution()\n","SDP = Solution()\n","\n","# start at target storage\n","DDP.simS[0,0] = Stargets[0]\n","SDP.simS[0,0] = Stargets[0]\n","\n","# vector of standard normal random variables for flow simulation\n","Z = np.zeros([nYears*nSeasons+1])\n","\n","# generate prior season's random normal inflow\n","seed = 0\n","np.random.seed(seed)\n","Z[seed] = ss.norm.rvs(0,1,1)[0]\n","Qpast = np.exp(Z[seed]*sigmaY[-1] + muY[-1])\n","\n","# simulate operations over 50 years of 3 seasons\n","for year in range(nYears):\n","  for season in range(nSeasons):\n","    # generate this season's inflow (set a seed to make it reproducible)\n","    seed += 1\n","    np.random.seed(seed)\n","    # generate flow conditional on previous season's flow and transform to real-space\n","    Z[seed] = rho*(Z[seed-1]) + ss.norm.rvs(0,1,1)[0]*np.sqrt(1-rho**2)\n","    Qnow = np.exp(Z[seed]*sigmaY[season] + muY[season])\n","\n","    # find DDP releases from its policy: interpolate release between nearest storages\n","    DDP.prescribedR = np.interp(DDP.simS[year,season],states,DDP_release_policy[:,season])\n","\n","    # find SDP releases from its policy: interpolate release between nearest storages and flows\n","    Ylevels1 = np.linspace(ss.norm.ppf(0.01,muY[season-1],sigmaY[season-1]),ss.norm.ppf(0.99,muY[season-1],sigmaY[season-1]),nLevels)\n","    f = interp2d((states, Ylevels1), SDP_release_policy[:,:,season])\n","    if np.log(Qpast) <= Ylevels1[0]: # interpolate between storages at lowest flow level\n","      SDP.prescribedR = np.interp(SDP.simS[year,season],states,SDP_release_policy[:,0,season])\n","    elif np.log(Qpast) >= Ylevels1[-1]: # interpolate between storages at highest flow level\n","      SDP.prescribedR = np.interp(SDP.simS[year,season],states,SDP_release_policy[:,-1,season])\n","    else: # interpolate over 2-D grid\n","      SDP.prescribedR = f(np.array([SDP.simS[year,season],np.log(Qpast)]))[0]\n","\n","    # find actual release (what is physically possible) and calculate storage from mass balance\n","    DDP.getSimStates(Qnow, year, season)\n","    SDP.getSimStates(Qnow, year, season)\n","\n","    # update past flow for next season's calculation\n","    Qpast = Qnow\n","\n","  # calculate total cost (squared deviations from targets) in simulated year\n","  DDP.getSimCosts(year)\n","  SDP.getSimCosts(year)"],"metadata":{"id":"L1o7ZCuBYzgE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DDP vs. SDP Comparison over Simulation\n","\n","Based on your simulation from part (c), make a 3x1 panel figure of the empirical cumulative distribution function of total squared deviations from the storage target in one panel, from the release target in another panel, and the total across both in the third panel. Do this for the policies from parts (a) and (b) using a different color for each. Discuss the differences you see in performance between the operating policies found using DDP vs. SDP and why."],"metadata":{"id":"5S_swDknT2V_"}},{"cell_type":"code","source":["# cumulative probabilities\n","p = np.arange(1,nYears+1,1) / (nYears+1)\n","\n","# plot ECDF of R, S and total deviations for each policy\n","fig = plt.figure(figsize=[12,4])\n","\n","# make list of things to loop through for each plot\n","DP_costs = [DDP.S_costs, DDP.R_costs, DDP.Total_costs]\n","SDP_costs = [SDP.S_costs, SDP.R_costs, SDP.Total_costs]\n","xlabels = [\"Squared Storage Deviations\", \"Squared Release Deviations\", \"Total Squared Deviations\"]\n","\n","for i in range(len(DP_costs)):\n","  ax = fig.add_subplot(1,3,i+1)\n","  # step-function of sorted deviations from target for each algorithm\n","  l1, = ax.step(np.sort(DP_costs[i]),p,color=\"tab:blue\",linewidth=2)\n","  l2, = ax.step(np.sort(SDP_costs[i]),p,color=\"tab:green\",linewidth=2)\n","  ax.set_xlabel(xlabels[i], fontsize=16)\n","  ax.set_ylabel(\"Cumulative Probability\", fontsize=16)\n","  ax.tick_params(axis=\"both\",labelsize=14)\n","\n","ax.legend([l1, l2],[\"DDP\",\"SDP\"],fontsize=16,loc=\"lower right\")\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"Zd4DFnYGUDAt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SDP far outperforms DDP on squared storage deviations, but does slightly worse on squared release deviations. Overall, though, it has lower total squared deviations."],"metadata":{"id":"H9w1acNmWU0Q"}},{"cell_type":"markdown","source":["Plot the number of violations of Rmin and Rmax from DDP and SDP over the course of the simulation."],"metadata":{"id":"NobB17rsfnXe"}},{"cell_type":"code","source":["violations = (\"Rmin\", \"Rmax\")\n","solutions = {\n","    'DDP': (DDP.Rmin_violations, DDP.Rmax_violations),\n","    'SDP': (SDP.Rmin_violations, SDP.Rmax_violations),\n","}\n","\n","x = np.arange(len(violations))  # the label locations\n","width = 0.25  # the width of the bars\n","multiplier = 0\n","\n","fig, ax = plt.subplots(layout='constrained')\n","\n","for attribute, measurement in solutions.items():\n","    offset = width * multiplier\n","    rects = ax.bar(x + offset, measurement, width, label=attribute)\n","    multiplier += 1\n","\n","# Add some text for labels, title and custom x-axis tick labels, etc.\n","ax.set_ylabel('Number of Violations')\n","ax.set_title('Release Violations')\n","ax.set_xticks(x + width, violations)\n","ax.legend(loc='upper left')"],"metadata":{"id":"qIEWF3jkfr79"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DDP has one violation of Rmin and Rmax, while SDP has no violations of Rmin and one of Rmax."],"metadata":{"id":"K4fYuy9Xgg62"}}]}