{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFr4XZxwtYx+hM/lmx4fZP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Non-convex Nonlinear Programming"],"metadata":{"id":"Li6FCVqHTl0B"}},{"cell_type":"markdown","source":["cvxpy requires convex objectives and constraints. If that's not the case, for example, if we have a black-box model, we need to use other NLP algorithms. scipy has several such solvers. We'll compare scipy.optimize.minimize with different gradient-based solvers using multi-start and scipy.optimize.differential_evolution, an evolutionary algorithm, on a hydrological model calibration problem."],"metadata":{"id":"LKb7dUHiTsL-"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from scipy.optimize import minimize, differential_evolution"],"metadata":{"id":"k7ppbBD9VloW","executionInfo":{"status":"ok","timestamp":1705901776784,"user_tz":300,"elapsed":279,"user":{"displayName":"Julianne Quinn","userId":"01910763653616567327"}}},"execution_count":261,"outputs":[]},{"cell_type":"markdown","source":["We'll use the simple hydrological model hymod on a dataset of one year from the Rivanna River at Palmyra in from HY 2011-2023. We'll use the dataretrieval package to get the streamflow data from USGS (https://github.com/DOI-USGS/dataretrieval-python) and the pyncei package to get the climate data from NOAA (https://github.com/adamancer/pyncei)."],"metadata":{"id":"qSPhoZAWYhKZ"}},{"cell_type":"code","source":["!pip install dataretrieval\n","!pip install pyncei"],"metadata":{"id":"aQVi1jK8OF2G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import dataretrieval.nwis as nwis\n","flow_df = nwis.get_record(sites='02034000', service='dv', start='2010-10-01', end='2023-09-30') # Rivanna River at Palmyra in cfs\n","flow_df.head()"],"metadata":{"id":"bW5tzGdCOSph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before downloading the NOAA climate data, you'll need to get your own token here: https://www.ncdc.noaa.gov/cdo-web/token. Replace my token with your own."],"metadata":{"id":"FH2bXblzPhvL"}},{"cell_type":"code","source":["from pyncei import NCEIBot\n","#ncei = NCEIBot(\"ExampleNCEIAPIToken\")\n","ncei = NCEIBot(\"klhIxphHLKDrrnjPtJGNpfEnfbERfUvZ\")"],"metadata":{"id":"fZTE1TuMPlQA","executionInfo":{"status":"ok","timestamp":1705899223346,"user_tz":300,"elapsed":194,"user":{"displayName":"Julianne Quinn","userId":"01910763653616567327"}}},"execution_count":213,"outputs":[]},{"cell_type":"code","source":["# loop through each year and append to data frame because you can't download more than 1 year at a time\n","for year in range(2010,2023):\n","  response = ncei.get_data(\n","      datasetid=\"GHCND\",\n","      stationid=[\"GHCND:USC00441593\"],# Charlottesville, VA; should really use average over basin\n","      datatypeid=[\"TOBS\", \"TMAX\",\"TMIN\", \"PRCP\"], # degrees Celsius, mm\n","      startdate=str(year) + \"-10-01\",\n","      enddate=str(year+1) + \"-09-30\",\n","  )\n","  if year == 2010:\n","    climate_df = response.to_dataframe()\n","  else:\n","    climate_df = pd.concat([climate_df,response.to_dataframe()])\n","\n","climate_df.head()"],"metadata":{"id":"3ClZdtFCQQ2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reformat the data so each variable is stored in a separate dataframe with the date"],"metadata":{"id":"EBEsfNx5PUpL"}},{"cell_type":"code","source":["from datetime import datetime\n","\n","def make_table(table_in, col_str):\n","  data_table = pd.DataFrame()\n","  data_table[col_str] = table_in['value'].iloc[np.where(table_in['datatype']==col_str)[0]]\n","  data_table['date'] = table_in['date'].iloc[np.where(table_in['datatype']==col_str)[0]]\n","  data_table = data_table.reset_index()\n","  data_table['date'] = [datetime.strftime(dt, \"%Y-%m-%d\") for dt in data_table['date']]\n","  data_table.index = data_table['date']\n","  data_table = data_table.drop(columns=['date','index'])\n","  return data_table\n","\n","data_P = make_table(climate_df, \"PRCP\")\n","data_T = make_table(climate_df, \"TOBS\")\n","data_Tmax = make_table(climate_df, \"TMAX\")\n","data_Tmin = make_table(climate_df, \"TMIN\")\n","\n","data_Q = pd.DataFrame()\n","data_Q['Q'] = flow_df['00060_Mean']\n","data_Q['date'] = [datetime.strftime(dt, \"%Y-%m-%d\") for dt in data_Q.index]\n","#data_Q = data_Q.reset_index()\n","#data_Q = data_Q.drop(columns=['datetime'])\n","data_Q.index = data_Q['date']\n","data_Q = data_Q.drop(columns=['date'])\n","\n","print(data_P.head())\n","print(data_T.head())\n","print(data_Tmax.head())\n","print(data_Tmin.head())\n","print(data_Q.head())\n","\n","print(len(data_P))\n","print(len(data_T))\n","print(len(data_Tmax))\n","print(len(data_Tmin))\n","print(len(data_Q))"],"metadata":{"id":"PIU76O2nZ6QV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The datasets are different lengths, so some variables must have missing values. Join the data frames and fill the missing values via interpolation."],"metadata":{"id":"UsGZMEurP3VB"}},{"cell_type":"code","source":["data = data_Q.join(data_P)\n","data = data.join(data_Tmax)\n","data = data.join(data_Tmin)\n","data = data.join(data_T)\n","print(data.isna().sum())\n","data.interpolate('linear',axis=0,inplace=True)\n","print(data.isna().sum())\n","data.index = pd.to_datetime(data.index)"],"metadata":{"id":"YnGZ2JSRMw61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Estimate PET using Hargreaves because it only requires temperature and latitude (and therefore isn't the best approach, but we're using a simple example). Code for PET calculation is available here: https://github.com/pyet-org/pyet"],"metadata":{"id":"tL0V-DjCLnzp"}},{"cell_type":"code","source":["!pip install pyet"],"metadata":{"id":"1rdqu2u4btBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lat = 37.8579192*np.pi/180.0 # latitude of Palmyra in radians (should really use latitude at centroid of basin)\n","elevation = 209.69/3.28084 # elevation of Palmyra gauge in feet converted to meters\n","area = 663*5280**2 # drainage area of Rivanna River at Palmyra in square miles converted to square ft\n","\n","import pyet\n","data['PET'] = pyet.hargreaves(tmean=data['TOBS'], tmax=data['TMAX'], tmin=data['TMIN'], lat=lat)\n","data.head()"],"metadata":{"id":"3n2c6J_MLi_u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Convert data frames to numpy arrays to run HYMOD"],"metadata":{"id":"Kgt0uSWyXtQE"}},{"cell_type":"code","source":["data_Q = np.array(data['Q'])*3600*24*1000/3.28/area # convert cfs to mm/day\n","data_P = np.array(data['PRCP']) # mm/day\n","data_PET = np.array(data['PET']) # mm/day\n","ndays = len(data_Q)\n","\n","print(data_Q)"],"metadata":{"id":"Ec7AsmnOXaUE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["HYMOD function"],"metadata":{"id":"-qChU4s7XxB9"}},{"cell_type":"code","source":["def hymod(x, mode='optimize'):\n","\n","    # assign parameters\n","    Sm_max,B,alpha,Kf,Ks = list(x)\n","\n","    # initialize storage, all empty to start\n","    Sm,Sf1,Sf2,Sf3,Ss1 = [np.zeros(ndays) for _ in range(5)]\n","    Q = np.zeros(ndays)\n","\n","    for t in range(1,ndays):\n","\n","        # calculate all fluxes\n","        P = data_P[t]\n","        Peff = P*(1 - max(1-Sm[t-1]/Sm_max,0)**B) # PDM model Moore 1985\n","        Evap = min(data_PET[t]*(Sm[t-1]/Sm_max), Sm[t-1])\n","\n","        Qf1 = Kf*Sf1[t-1]\n","        Qf2 = Kf*Sf2[t-1]\n","        Qf3 = Kf*Sf3[t-1]\n","        Qs1 = Ks*Ss1[t-1]\n","\n","        # update state variables\n","        Sm[t] = Sm[t-1] + P - Peff - Evap\n","        Sf1[t] = Sf1[t-1] + alpha*Peff - Qf1\n","        Sf2[t] = Sf2[t-1] + Qf1 - Qf2\n","        Sf3[t] = Sf3[t-1] + Qf2 - Qf3\n","        Ss1[t] = Ss1[t-1] + (1-alpha)*Peff - Qs1\n","\n","        Q[t] = Qs1 + Qf3\n","\n","    # remove first 3 years as burn-in\n","    Q = Q[(365*3)::]\n","\n","    if mode=='simulate':\n","        return Q # simulated flow\n","    else:\n","        return np.sqrt(np.sum((np.log(Q+0.001)-np.log(data_Q[(365*3)::]+0.001))**2)) # RMSE in log-space\n","        #np.sqrt(np.sum((Q-data_Q[365*3::])**2))"],"metadata":{"id":"MRdaK56DW62Z","executionInfo":{"status":"ok","timestamp":1705903347136,"user_tz":300,"elapsed":278,"user":{"displayName":"Julianne Quinn","userId":"01910763653616567327"}}},"execution_count":304,"outputs":[]},{"cell_type":"markdown","source":["Generate 10 initial starting points from a Latin hypercube sample."],"metadata":{"id":"LrV1VDCgs5XK"}},{"cell_type":"code","source":["import scipy.stats as ss\n","sampler = ss.qmc.LatinHypercube(d=5)\n","sample = sampler.random(n=10)\n","l_bounds = np.zeros([5])\n","u_bounds = np.array([500,2,1,1,0.1])\n","x0 = ss.qmc.scale(sample, l_bounds, u_bounds)\n","x0"],"metadata":{"id":"G8GRi0HvYVIg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Optimize with differential evolution using the Latin hypercube sample as the initial population. See more about arguments here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html"],"metadata":{"id":"vD4nAd9QYraB"}},{"cell_type":"code","source":["bounds = [(0, 500), (0, 2), (0, 1), (0, 1), (0, 0.1)]\n","result = differential_evolution(hymod, bounds=bounds, init=x0, seed=1, maxiter=1000)\n","print(result)"],"metadata":{"id":"ue2Cnwansxr8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Store best parameter set and corresponding log-space RMSE"],"metadata":{"id":"_YI5G3ONuihu"}},{"cell_type":"code","source":["DE_x_best = result.x\n","DE_obj_best = result.fun"],"metadata":{"id":"1mDLhS7muod_","executionInfo":{"status":"ok","timestamp":1705903372477,"user_tz":300,"elapsed":175,"user":{"displayName":"Julianne Quinn","userId":"01910763653616567327"}}},"execution_count":306,"outputs":[]},{"cell_type":"markdown","source":["Optimize with scipy minimize, which defaults to BFGS, L-BFGS-B, SLSQP, depending on whether or not the problem has constraints or bounds. Use multi-start, initiating each search with one of the Latin hypercube samples. See more here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html."],"metadata":{"id":"FuaakT6zcHJ5"}},{"cell_type":"code","source":["x_trial = np.zeros([10,5])\n","RMSE_trial = np.zeros(10)\n","for i in range(10):\n","  result = minimize(hymod, x0[i,:], bounds=bounds)\n","  x_trial[i,:] = result.x\n","  RMSE_trial[i] = result.fun\n","\n","gradient_obj_best = np.min(RMSE_trial)\n","gradient_x_best = x_trial[np.argmin(RMSE_trial)]\n","\n","print(gradient_obj_best)\n","print(gradient_x_best)"],"metadata":{"id":"tk8_0gZFebcG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compare observations vs. simulations from best DE solution and best gradient-based solution"],"metadata":{"id":"V-76Ys3_vf7K"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","sim_Q_DE = hymod(DE_x_best, mode='simulate')\n","sim_Q_gradient = hymod(gradient_x_best, mode='simulate')\n","obs_Q = data_Q[(365*3)::]\n","\n","# make each year its own subplot\n","fig = plt.figure()\n","for i in range(10): # last 10 years\n","  ax = fig.add_subplot(5,2,i+1)\n","  ax.plot(obs_Q[(i*365):((i+1)*365)],c=\"black\",linewidth=2,label=\"Observations\")\n","  ax.plot(sim_Q_DE[(i*365):((i+1)*365)],c=\"tab:red\",linewidth=2,label=\"DE Simulations\")\n","  ax.plot(sim_Q_gradient[(i*365):((i+1)*365)],c=\"tab:blue\",linewidth=2,label=\"Gradient Simulations\")\n","\n","fig.tight_layout()\n","handles, labels = ax.get_legend_handles_labels()\n","fig.subplots_adjust(bottom=0.2)\n","fig.legend(handles, labels, fontsize=16,loc='lower center',ncol=3)\n","fig.show()\n"],"metadata":{"id":"nZmov8_fvlQv"},"execution_count":null,"outputs":[]}]}