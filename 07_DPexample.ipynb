{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJ/by7trifeGoAa14bhB81"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Dynamic Programming for reservoir operations vs. Non-Linear Programming\n","\n","## Part a\n","\n","Consider a three-season reservoir operation problem. Assume the inflows are always 10, 50 and 30 in seasons 1, 2 and 3, respectively. Your goal is to minimize the sum of total squared deviations above and below a constant storage target of 20 and above and below a constant release target of 20 in each of the three seasons.\n","\n","Using nonlinear programming with the BFGS algorithm in scipy.minimize.optimize, find the optimal release in each season. Use bounds of [0,40] for releases and [0,30] for storage."],"metadata":{"id":"mvaDahOKNyNA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIE7ok8oaphe"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy.optimize import minimize\n","import matplotlib.pyplot as plt\n","\n","######################## NLP Optimization ########################\n","\n","# inflows and targets\n","Qs = np.array([10, 50, 30])\n","nSeasons = len(Qs)\n","Stargets = np.ones([nSeasons])*20 # target of 20 every period\n","Rtargets = np.ones([nSeasons])*20 # target of 20 every period\n","\n","# reservoir parameters\n","K = 30 # reservoir capacity\n","Rmax = 40 # maximum possible release\n","Rmin = 10 # minimum possible release\n","\n","def calcCostNLP(x):\n","    # convert decision variables to state variables\n","    # nSeasons does not need to be passed to this function as it is defined\n","    # as a global variable before it\n","    R = x[0:nSeasons]\n","    S = x[nSeasons::]\n","\n","    # calculate squared differences between actual values and targets\n","    # Stargets and Rtargets do not need to be passed to this function as they\n","    # are defined as global variables before it\n","    S_deviation = np.sum((S-Stargets)**2)\n","    R_deviation = np.sum((R-Rtargets)**2)\n","    Cost = S_deviation + R_deviation\n","\n","    return Cost\n","\n","constraints = ({'type': 'eq', 'fun': lambda x: x[4] - x[3] - Qs[0] + x[0]}, # mass balance t=0->1\n","        {'type': 'eq', 'fun': lambda x: x[5] - x[4] - Qs[1] + x[1]}, # mass balance t=1->2\n","        {'type': 'eq', 'fun': lambda x: x[3] - x[5] - Qs[2] + x[2]}) # mass balance t=2->3, t3=t0\n","\n","\n","# R is between Rmin and Rmax, S is positive and can't exceed capacity K\n","bounds = np.concatenate(([[Rmin,Rmax]]*3, [[0,K]]*3),0).tolist()\n","\n","# initialize guesses at R = R_target and S = S_target for all stages\n","x0 = np.concatenate((Rtargets,Stargets),0)\n","\n","# optimization with BFGS\n","result = minimize(calcCostNLP, x0, bounds=bounds, constraints=constraints)\n","print(\"Optimal releases each season: \", result.x[0:3]) # first three are optimal releases, next three are optimal storages\n"]},{"cell_type":"markdown","source":["## Part b\n","\n","Using Python, write a discrete dynamic program to find the operating policy. Consider 7 discrete storage values: 0, 5, 10, 15, 20, 25, and 30. Assume the releases cannot be less than 10 or greater than 40. Report your results as a table showing the optimal release in each of the three seasons for each of the seven storage levels."],"metadata":{"id":"v7TkupcQShRx"}},{"cell_type":"code","source":["######################## DP Optimization ########################\n","\n","def calcCostDP(S, Q, Starget, Rtarget, bounds, FutureCost):\n","    '''\n","    Function to calculate the optimal release (Rbest) from each storage state (S)\n","    and associated present and future cost (Cbest), defined as the total squared\n","    deviation between storage and release targets for that stage.\n","\n","    Inputs:\n","      S: array of discrete storage values representing the states\n","      Q: inflow received at this stage\n","      Starget: target storage for next stage\n","      Rtarget: release target for this stage\n","      bounds: bounds on possible releases\n","      FutureCost: array of future costs at each state that will be added to\n","      cost of the optimal state transition at this stage to compute present + future cost\n","\n","    Outputs:\n","      Rbest: array of optimal releases from each state in S\n","      Cbest: array of present + future costs associated with each state S\n","    '''\n","\n","    # initialize current cost at infinity and releases at 0\n","    Cbest = np.empty([len(S)])+np.inf\n","    Rbest = np.zeros([len(S)])\n","    for i, s in enumerate(S): # storage at stage t\n","        # find optimal storage to move to at stage t+1\n","        for j, sNext in enumerate(S): # storage at stage t+1\n","            R = s + Q - sNext # release to get to sNext\n","            # find cost of this release if it's feasible\n","            if R >= bounds[0] and R <= bounds[1]:\n","                # compute total cost C (total deviation from targets + future cost at sNext)\n","                S_deviation = (sNext-Starget)**2\n","                R_deviation = (R-Rtarget)**2\n","                C = S_deviation + R_deviation + FutureCost[j]\n","                # update optimal value (Cbest) and decision (Rbest) if better than current best\n","                if C < Cbest[i]:\n","                    Cbest[i] = C\n","                    Rbest[i] = R\n","\n","    return Rbest, Cbest\n","\n","# get indices of stages\n","nStages = nSeasons\n","forward_indices = np.arange(nStages)\n","backward_indices = forward_indices[::-1] # reverse order of stages for backward-moving DP\n","backward_indices = np.insert(backward_indices,0,0) # put 0 at beginning to make it cyclic\n","\n","# discretize states\n","states = np.arange(0,31,5)\n","nStates = len(states)\n","\n","# bounds on decision variables (releases)\n","bounds = np.array([Rmin, Rmax])\n","\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","costs = np.empty([nStates,nStages])\n","release_policy = np.empty([nStates,nStages])\n","\n","# initialize FutureCost at 0 for all states; will update as we move backwards\n","FutureCost = np.zeros([nStates])\n","\n","# begin backward-moving DP\n","loop = True\n","while loop:\n","    count = 0\n","    for index in backward_indices[0:-1]:\n","        # find optimal release and value of each state in this stage\n","        # states (storage targets) are for next period (index) while inflows and releases are for this period (index-1)\n","        R, FutureCost = calcCostDP(states, Qs[index-1], Stargets[index], Rtargets[index-1], bounds, FutureCost)\n","\n","        # count iterations with no change in optimal release\n","        if np.all(R == release_policy[:,index-1]):\n","            count += 1\n","\n","        # update best releases and value of each state\n","        costs[:,index] = FutureCost\n","        release_policy[:,index-1] = R\n","\n","    # stop loop if no change in optimal decisions across all iterations\n","    if count == len(backward_indices[0:-1]):\n","        break\n","\n","release_policy_df = pd.DataFrame(release_policy, columns=[\"Season 1\",\"Season 2\",\"Season 3\"],index=states)\n","release_policy_df.index.rename(\"Storage\",inplace=True)\n","release_policy_df"],"metadata":{"id":"3-ETD2wmOZyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part c\n","\n","Now consider that there is actually uncertainty in the reservoir inflows each season. The probability mass function (PMF) of discrete inflows observed each season are the following:\n","        \\begin{align}\n","            \\text{Season 1:} &\\quad p(Q=10) = 0.75, \\quad p(Q=20) = 0.25 \\\\\n","            \\text{Season 2:} &\\quad p(Q=40) = 0.25, \\quad p(Q=50) = 0.75 \\\\\n","            \\text{Season 3:} &\\quad p(Q=20) = 0.25, \\quad p(Q=30) = 0.5, \\quad p(Q=40) = 0.25\n","        \\end{align}\n","Simulate 50 years of operations in which the release each season is always that found to be optimal in part (a). Repeat this using the operating policy found to be optimal in part (b). In both cases, if there is insufficient water to meet the release prescribed by the policy, only release as much water as is available. Likewise, if the prescribed release would result in exceeding the reservoir capacity, release as much as needed to prevent that (this may exceed 40, but that's okay for the purpose of this simulation)."],"metadata":{"id":"cxc9BTJOTg7I"}},{"cell_type":"code","source":["######################## Simulation ########################\n","\n","# initialize storages and releases for simulation of 50 years of 3 seasons with NLP and DP policies\n","nYears = 50\n","\n","class Solution():\n","  def __init__(self):\n","    '''initialize Solution class with certain attributes for DP vs. NLP solution'''\n","    self.simS = np.zeros([nYears,nSeasons])\n","    self.simR = np.zeros([nYears,nSeasons])\n","    self.S_costs = np.zeros([nYears])\n","    self.R_costs = np.zeros([nYears])\n","    self.Total_costs = np.zeros([nYears])\n","    self.prescribedR = None\n","\n","  def getSimStates(self, Q, year, season):\n","    '''method of Solution class to calculate simulated R and S'''\n","    # adjust prescribed release if not physically possible\n","    # R = min(prescribedR, simS + Q) prevents it from releasing more water than is available\n","    # max(simS + Q - K, R) prevents storage capacity from being exceeded\n","    self.simR[year,season] = max(self.simS[year,season] + Q - K,\n","                           min(self.prescribedR, self.simS[year,season] + Q))\n","\n","    # calculate new storage\n","    if season != (nSeasons-1): # storage in next season of same year\n","      self.simS[year,season+1] = self.simS[year,season] + Q - self.simR[year,season]\n","    elif year != (nYears-1): # storage in season 1 of next year\n","      self.simS[year+1,0] = self.simS[year,season] + Q - self.simR[year,season]\n","\n","  def getSimCosts(self, year):\n","    '''method of Solution class to calculate cost (total deviation from targets) over simulation'''\n","    self.S_costs[year] = np.sum((self.simS[year,:] - Stargets)**2)\n","    self.R_costs[year] = np.sum((self.simR[year,:] - Rtargets)**2)\n","    self.Total_costs[year] = self.S_costs[year] + self.R_costs[year]"],"metadata":{"id":"56pNFTrGOx26","executionInfo":{"status":"ok","timestamp":1709144278327,"user_tz":300,"elapsed":173,"user":{"displayName":"Julianne Quinn","userId":"01910763653616567327"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# PMF of inflows each season\n","inflows = np.array([[0,10,20], # season 1\n","                 [40,50,60], # season 2\n","                 [20,30,40]]) # season 3\n","probs = np.array([[0,0.75,0.25],\n","                  [0.25,0.75,0],\n","                  [0.25,0.5,0.25]])\n","\n","# create objects of Solution class for NLP and DP solutions\n","DP = Solution()\n","NLP = Solution()\n","\n","# start at target storage\n","DP.simS[0,0] = Stargets[0]\n","NLP.simS[0,0] = Stargets[0]\n","\n","seed = 0\n","# simulate operations over 50 years of 3 seasons\n","for year in range(nYears):\n","    for season in range(nSeasons):\n","        # generate inflow (set a seed to make it reproducible)\n","        seed += 1\n","        np.random.seed(seed)\n","        Q = np.random.choice(inflows[season,:], 1, p=probs[season,:])[0]\n","\n","        # find DP release from policy\n","        DP.prescribedR = release_policy[np.where(states==DP.simS[year,season])[0][0],season]\n","        NLP.prescribedR = result.x[season]\n","\n","        # find actual release (what is physically possible) and calculate storage from mass balance\n","        DP.getSimStates(Q, year, season)\n","        NLP.getSimStates(Q, year, season)\n","\n","    # calculate total cost (squared deviations from targets) in simulated year\n","    DP.getSimCosts(year)\n","    NLP.getSimCosts(year)"],"metadata":{"id":"L1o7ZCuBYzgE","executionInfo":{"status":"ok","timestamp":1709144280348,"user_tz":300,"elapsed":118,"user":{"displayName":"Julianne Quinn","userId":"01910763653616567327"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Part d\n","\n","Based on your simulation from part (c), make a 3x1 panel figure of the empirical cumulative distribution function of total squared deviations from the storage target in one panel, the release target in another panel, and the total across both in the third panel. Do this for the policies from parts (a) and (b) using a different color for each. Discuss the differences you see in performance between the operating policies found using NLP vs. DP and why."],"metadata":{"id":"5S_swDknT2V_"}},{"cell_type":"code","source":["# cumulative probabilities\n","p = np.arange(1,nYears+1,1) / (nYears+1)\n","\n","# plot ECDF of R, S and total deviations for each policy\n","fig = plt.figure(figsize=[12,4])\n","\n","# make list of things to loop through for each plot\n","DP_costs = [DP.S_costs, DP.R_costs, DP.Total_costs]\n","NLP_costs = [NLP.S_costs, NLP.R_costs, NLP.Total_costs]\n","xlabels = [\"Squared Storage Deviations\", \"Squared Release Deviations\", \"Total Squared Deviations\"]\n","\n","for i in range(len(DP_costs)):\n","  ax = fig.add_subplot(1,3,i+1)\n","  # step-function of sorted deviations from target for each algorithm\n","  l1, = ax.step(np.sort(DP_costs[i]),p,color=\"tab:blue\",linewidth=2)\n","  l2, = ax.step(np.sort(NLP_costs[i]),p,color=\"tab:green\",linewidth=2)\n","  ax.set_xlabel(xlabels[i], fontsize=16)\n","  ax.set_ylabel(\"Cumulative Probability\", fontsize=16)\n","  ax.tick_params(axis=\"both\",labelsize=14)\n","\n","ax.legend([l1, l2],[\"DP\",\"NLP\"],fontsize=16,loc=\"lower right\")\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"Zd4DFnYGUDAt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DP equals or outperforms NLP on the storage deviations across the whole distribution. It outperforms NLP for the lower 50\\% of the release deviations distribution, but has larger deviations in the upper 50% of the distribution. Summing across the two, DP does better over all but the best ~5\\% years, significantly reducing the worst total deviations. This is because of its ability to adapt decisions at each stage if the state is not what was expected due to a different inflow than what was forecast."],"metadata":{"id":"H9w1acNmWU0Q"}}]}