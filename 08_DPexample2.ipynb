{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1AZ7ua8RPZA7nClzTQrAB9R1Lxh-uwXaP","timestamp":1772053879406}],"authorship_tag":"ABX9TyO5DT1rlFRI1FRBobTH//pz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Dynamic Programming for reservoir operations\n","\n","## Part a: DP without interpolation\n","\n","Consider a three-season reservoir operation problem. Assume the inflows are always 12, 48 and 33 in seasons 1, 2 and 3, respectively. Your goal is to minimize the sum of total squared deviations above and below a constant storage target of 20 and above and below a constant release target of 20 in each of the three seasons.\n","\n","Using Python, write a discrete dynamic program to find the operating policy. Consider 7 discrete storage values: 0, 5, 10, 15, 20, 25, and 30. Assume the releases cannot be less than 10 or greater than 40. Report your results as a table showing the optimal release in each of the three seasons for each of the seven storage levels."],"metadata":{"id":"mvaDahOKNyNA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIE7ok8oaphe"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from scipy.optimize import minimize\n","import matplotlib.pyplot as plt\n","\n","######################## DP Optimization without Interpolation ########################\n","\n","# inflows and targets\n","Qs = np.array([10, 50, 30])\n","nSeasons = len(Qs)\n","Stargets = np.ones([nSeasons])*20 # target of 20 every period\n","Rtargets = np.ones([nSeasons])*20 # target of 20 every period\n","\n","# reservoir parameters\n","K = 30 # reservoir capacity\n","Rmax = 40 # maximum possible release\n","Rmin = 10 # minimum possible release\n","\n","def calcCostDP(S, Q, Starget, Rtarget, bounds, FutureCost):\n","    '''\n","    Function to calculate the optimal release (Rbest) from each storage state (S)\n","    and associated present and future cost (Cbest), defined as the total squared\n","    deviation between storage and release targets for that stage.\n","\n","    Inputs:\n","      S: array of discrete storage values representing the states\n","      Q: inflow received at this stage\n","      Starget: target storage for next stage\n","      Rtarget: release target for this stage\n","      bounds: bounds on possible releases\n","      FutureCost: array of future costs at each state that will be added to\n","      cost of the optimal state transition at this stage to compute present + future cost\n","\n","    Outputs:\n","      Rbest: array of optimal releases from each state in S\n","      Cbest: array of present + future costs associated with each state S\n","    '''\n","\n","    # initialize current cost at infinity and releases at 0\n","    Cbest = np.empty([len(S)])+np.inf\n","    Rbest = np.zeros([len(S)])\n","    for i, s in enumerate(S): # storage at stage t\n","        # find optimal storage to move to at stage t+1\n","        for j, sNext in enumerate(S): # storage at stage t+1\n","            R = s + Q - sNext # release to get to sNext\n","            # find cost of this release if it's feasible\n","            if R >= bounds[0] and R <= bounds[1]:\n","                # compute total cost C (total deviation from targets + future cost at sNext)\n","                S_deviation = (sNext-Starget)**2\n","                R_deviation = (R-Rtarget)**2\n","                C = S_deviation + R_deviation + FutureCost[j]\n","                # update optimal value (Cbest) and decision (Rbest) if better than current best\n","                if C < Cbest[i]:\n","                    Cbest[i] = C\n","                    Rbest[i] = R\n","\n","    return Rbest, Cbest\n","\n","# get indices of stages\n","nStages = nSeasons\n","forward_indices = np.arange(nStages)\n","backward_indices = forward_indices[::-1] # reverse order of stages for backward-moving DP\n","backward_indices = np.insert(backward_indices,0,0) # put 0 at beginning to make it cyclic\n","\n","# discretize states\n","states = np.arange(0,31,5)\n","nStates = len(states)\n","\n","# bounds on decision variables (releases)\n","bounds = np.array([Rmin, Rmax])\n","\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","costs = np.empty([nStates,nStages])\n","release_policy = np.empty([nStates,nStages])\n","\n","# initialize FutureCost at 0 for all states; will update as we move backwards\n","FutureCost = np.zeros([nStates])\n","\n","# begin backward-moving DP\n","loop = True\n","while loop:\n","    count = 0\n","    for index in backward_indices[0:-1]:\n","        # find optimal release and value of each state in this stage\n","        # states (storage targets) are for next period (index) while inflows and releases are for this period (index-1)\n","        R, FutureCost = calcCostDP(states, Qs[index-1], Stargets[index], Rtargets[index-1], bounds, FutureCost)\n","\n","        # count iterations with no change in optimal release\n","        if np.all(R == release_policy[:,index-1]):\n","            count += 1\n","\n","        # update best releases and value of each state\n","        costs[:,index] = FutureCost\n","        release_policy[:,index-1] = R\n","\n","    # stop loop if no change in optimal decisions across all iterations\n","    if count == len(backward_indices[0:-1]):\n","        break\n","\n","release_policy_df = pd.DataFrame(release_policy, columns=[\"Season 1\",\"Season 2\",\"Season 3\"],index=states)\n","release_policy_df.index.rename(\"Storage\",inplace=True)\n","release_policy_df\n"]},{"cell_type":"markdown","source":["## Part b: DP with interpolation\n","\n","Repeat the optimization above, but allowing for releases that result in landing between states, using interpolation of the future value function to determine if that is best."],"metadata":{"id":"v7TkupcQShRx"}},{"cell_type":"code","source":["######################## DP Optimization with Interpolation ########################\n","\n","def findBestR(R, sCurrent, States, Starget, Rtarget, Q, FutureCost):\n","    # get first element of vector of length 1 of decision variables\n","    R = R[0]\n","\n","    # find what state that release would result in\n","    sNext = sCurrent + Q - R\n","\n","    # calculate the future cost at that state through interpolation\n","    # and add to the current cost\n","    CurrentCost = (R - Rtarget)**2 + (sNext - Starget)**2\n","    TotalCost = CurrentCost  + np.interp(sNext, States, FutureCost)\n","\n","    return TotalCost\n","\n","def calcCostDP_Interp(S, Q, Starget, Rtarget, bounds, FutureCost):\n","    '''\n","    Function to calculate the optimal release (Rbest) from each storage state (S)\n","    and associated present and future cost (Cbest), defined as the total squared\n","    deviation between storage and release targets for that stage.\n","\n","    Inputs:\n","      S: array of discrete storage values representing the states\n","      Q: inflow received at this stage\n","      Starget: target storage for next stage\n","      Rtarget: release target for this stage\n","      bounds: bounds on possible releases\n","      FutureCost: array of future costs at each state that will be added to\n","      cost of the optimal state transition at this stage to compute present + future cost\n","\n","    Outputs:\n","      Rbest: array of optimal releases from each state in S\n","      Cbest: array of present + future costs associated with each state S\n","    '''\n","\n","    # initialize current cost at infinity and releases at 0\n","    Cbest = np.empty([len(S)])+np.inf\n","    Rbest = np.zeros([len(S)])\n","    for i, s in enumerate(S): # storage at stage t\n","        # find optimal release from this state\n","        constraints = ({'type': 'ineq', 'fun': lambda x: K - (s + Q - x[0])}, # capacity - Snext >=0\n","                       {'type': 'ineq', 'fun': lambda x: s + Q - x[0]}) # Snext >= 0\n","        result = minimize(findBestR, x0=Rtarget, bounds=[[Rmin, Rmax]], constraints=constraints,\n","                          args=(s, S, Starget, Rtarget, Q, FutureCost))\n","        Rbest[i] = result.x[0]\n","        Cbest[i] = result.fun\n","\n","    return Rbest, Cbest\n","\n","# initialize matrices with costs of each state at each stage\n","# and optimal releases to make from each state at each stage\n","costs2 = np.empty([nStates,nStages])\n","release_policy2 = np.empty([nStates,nStages])\n","\n","# initialize FutureCost at 0 for all states; will update as we move backwards\n","FutureCost2 = np.zeros([nStates])\n","\n","# begin backward-moving DP\n","loop = True\n","tolerance = 0.1 # average % difference in optimal releases from one cycle ot the next at which to stop looping\n","avgPctDiff = np.inf\n","cycle = 0 # number of times cycling through all seasons\n","while loop:\n","    count = 0\n","    cycle += 1\n","    for index in backward_indices[0:-1]:\n","        # find optimal release and value of each state in this stage\n","        # states (storage targets) are for next period (index) while inflows and releases are for this period (index-1)\n","        R, FutureCost2 = calcCostDP_Interp(states, Qs[index-1], Stargets[index], Rtargets[index-1], bounds, FutureCost2)\n","\n","        # find average % difference in optimal releases at this stage compared to the last loop\n","        if cycle > 1:\n","          avgPctDiff = np.mean(np.abs(R - release_policy2[:,index-1])*100 / release_policy2[:,index-1])\n","          print(avgPctDiff)\n","          if avgPctDiff < tolerance:\n","            count += 1\n","\n","        # update best releases and value of each state\n","        costs2[:,index] = FutureCost2\n","        release_policy2[:,index-1] = R\n","\n","    if count == len(backward_indices[0:-1]):\n","        break\n","\n","release_policy2_df = pd.DataFrame(release_policy2, columns=[\"Season 1\",\"Season 2\",\"Season 3\"],index=states)\n","release_policy2_df.index.rename(\"Storage\",inplace=True)\n","release_policy2_df"],"metadata":{"id":"3-ETD2wmOZyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part c\n","\n","Now consider that there is actually uncertainty in the reservoir inflows each season. The probability mass function (PMF) of discrete inflows observed each season are the following:\n","        \\begin{align}\n","            \\text{Season 1:} &\\quad p(Q=10) = 0.75, \\quad p(Q=20) = 0.25 \\\\\n","            \\text{Season 2:} &\\quad p(Q=40) = 0.25, \\quad p(Q=50) = 0.75 \\\\\n","            \\text{Season 3:} &\\quad p(Q=20) = 0.25, \\quad p(Q=30) = 0.5, \\quad p(Q=40) = 0.25\n","        \\end{align}\n","Simulate 50 years of operations in which the release each season is based on the release policy found in part (a) vs. part (b). In both cases, if there is insufficient water to meet the release prescribed by the policy, only release as much water as is available. Likewise, if the prescribed release would result in exceeding the reservoir capacity, release as much as needed to prevent that. This may result in the release falling below 10 or exceeding 40, but that's okay for the purpose of this simulation."],"metadata":{"id":"cxc9BTJOTg7I"}},{"cell_type":"code","source":["######################## Simulation ########################\n","\n","# initialize storages and releases for simulation of 50 years of 3 seasons with NLP and DP policies\n","nYears = 50\n","\n","class Solution():\n","  def __init__(self):\n","    '''initialize Solution class with certain attributes for DP vs. NLP solution'''\n","    self.simS = np.zeros([nYears,nSeasons])\n","    self.simR = np.zeros([nYears,nSeasons])\n","    self.S_costs = np.zeros([nYears])\n","    self.R_costs = np.zeros([nYears])\n","    self.Total_costs = np.zeros([nYears])\n","    self.prescribedR = None\n","\n","  def getSimStates(self, Q, year, season):\n","    '''method of Solution class to calculate simulated R and S'''\n","    # adjust prescribed release if not physically possible\n","    # R = min(prescribedR, simS + Q) prevents it from releasing more water than is available\n","    # max(simS + Q - K, R) prevents storage capacity from being exceeded\n","    self.simR[year,season] = max(self.simS[year,season] + Q - K,\n","                           min(self.prescribedR, self.simS[year,season] + Q))\n","\n","    # calculate new storage\n","    if season != (nSeasons-1): # storage in next season of same year\n","      self.simS[year,season+1] = self.simS[year,season] + Q - self.simR[year,season]\n","    elif year != (nYears-1): # storage in season 1 of next year\n","      self.simS[year+1,0] = self.simS[year,season] + Q - self.simR[year,season]\n","\n","  def getSimCosts(self, year):\n","    '''method of Solution class to calculate cost (total deviation from targets) over simulation'''\n","    self.S_costs[year] = np.sum((self.simS[year,:] - Stargets)**2)\n","    self.R_costs[year] = np.sum((self.simR[year,:] - Rtargets)**2)\n","    self.Total_costs[year] = self.S_costs[year] + self.R_costs[year]"],"metadata":{"id":"56pNFTrGOx26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PMF of inflows each season\n","inflows = np.array([[0,10,20], # season 1\n","                 [40,50,60], # season 2\n","                 [20,30,40]]) # season 3\n","probs = np.array([[0,0.75,0.25],\n","                  [0.25,0.75,0],\n","                  [0.25,0.5,0.25]])\n","\n","# create objects of Solution class for NLP and DP solutions\n","DP1 = Solution()\n","DP2 = Solution()\n","\n","# start at target storage\n","DP1.simS[0,0] = Stargets[0]\n","DP2.simS[0,0] = Stargets[0]\n","\n","seed = 0\n","# simulate operations over 50 years of 3 seasons\n","for year in range(nYears):\n","    for season in range(nSeasons):\n","        # generate inflow (set a seed to make it reproducible)\n","        seed += 1\n","        np.random.seed(seed)\n","        Q = np.random.choice(inflows[season,:], 1, p=probs[season,:])[0]\n","\n","        # find DP release from policy\n","        DP1.prescribedR = release_policy[np.where(states==DP1.simS[year,season])[0][0],season]\n","        DP2.prescribedR = np.interp(DP2.simS[year,season],states,release_policy2[:,season])\n","\n","        # find actual release (what is physically possible) and calculate storage from mass balance\n","        DP1.getSimStates(Q, year, season)\n","        DP2.getSimStates(Q, year, season)\n","\n","    # calculate total cost (squared deviations from targets) in simulated year\n","    DP1.getSimCosts(year)\n","    DP2.getSimCosts(year)"],"metadata":{"id":"L1o7ZCuBYzgE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part d\n","\n","Based on your simulation from part (c), make a 3x1 panel figure of the empirical cumulative distribution function of total squared deviations from the storage target in one panel, from the release target in another panel, and the total across both in the third panel. Do this for the policies from parts (a) and (b) using a different color for each. Discuss the differences you see in performance between the operating policies found using NLP vs. DP and why."],"metadata":{"id":"5S_swDknT2V_"}},{"cell_type":"code","source":["# cumulative probabilities\n","p = np.arange(1,nYears+1,1) / (nYears+1)\n","\n","# plot ECDF of R, S and total deviations for each policy\n","fig = plt.figure(figsize=[12,4])\n","\n","# make list of things to loop through for each plot\n","DP1_costs = [DP1.S_costs, DP1.R_costs, DP1.Total_costs]\n","DP2_costs = [DP2.S_costs, DP2.R_costs, DP2.Total_costs]\n","xlabels = [\"Squared Storage Deviations\", \"Squared Release Deviations\", \"Total Squared Deviations\"]\n","\n","for i in range(len(DP1_costs)):\n","  ax = fig.add_subplot(1,3,i+1)\n","  # step-function of sorted deviations from target for each algorithm\n","  l1, = ax.step(np.sort(DP1_costs[i]),p,color=\"tab:blue\",linewidth=2)\n","  l2, = ax.step(np.sort(DP2_costs[i]),p,color=\"tab:green\",linewidth=2)\n","  ax.set_xlabel(xlabels[i], fontsize=16)\n","  ax.set_ylabel(\"Cumulative Probability\", fontsize=16)\n","  ax.tick_params(axis=\"both\",labelsize=14)\n","\n","ax.legend([l1, l2],[\"DP w/o Interpolation\",\"DP w/ Interpolation\"],fontsize=16,loc=\"lower right\")\n","fig.tight_layout()\n","fig.show()"],"metadata":{"id":"Zd4DFnYGUDAt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["DP with interpolation slightly outperforms DP without interpolation, as shown by the CDFs being generally to left, i.e. having probabilistically lower squared deviations. We can also compare the averages, calculated below."],"metadata":{"id":"H9w1acNmWU0Q"}},{"cell_type":"code","source":["AvgCostTable = pd.DataFrame({'Storage': [np.mean(DP1_costs[0]), np.mean(DP2_costs[0])],\n","                             'Release': [np.mean(DP1_costs[1]), np.mean(DP2_costs[1])],\n","                             'Total': [np.mean(DP1_costs[2]), np.mean(DP2_costs[2])]}, index=[\"DP w/o Interpolation\",\"DP w/ Interpolation\"])\n","AvgCostTable.index.rename(\"Policy\",inplace=True)\n","\n","AvgCostTable"],"metadata":{"id":"IOlEvNv-sIVV"},"execution_count":null,"outputs":[]}]}